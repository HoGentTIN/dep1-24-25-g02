{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: DimDate Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade sqlalchemy\n",
    "%pip install --upgrade pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "print(pyodbc.drivers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gegevens voor de verbinding\n",
    "server = r\"localhost\"  # Servernaam of IP-adres van je SQL Server\n",
    "database = \"DEP1_DWH\"  # Naam van je database\n",
    "\n",
    "# Maak de verbindingsstring met Windows Authenticatie (Integrated Security)\n",
    "engine = create_engine(\"mssql+pyodbc://@{}/{}?driver=ODBC+Driver+17+for+SQL+Server\".format(server, database))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een range van datums\n",
    "date_list = pd.date_range(start=\"01-01-2010\", end=\"31-12-2025\", freq='D')\n",
    "\n",
    "months_translation = {\n",
    "    'January': 'Januari', 'February': 'Februari', 'March': 'Maart', 'April': 'April',\n",
    "    'May': 'Mei', 'June': 'Juni', 'July': 'Juli', 'August': 'Augustus', \n",
    "    'September': 'September', 'October': 'Oktober', 'November': 'November', 'December': 'December'\n",
    "}\n",
    "\n",
    "days_translation = {\n",
    "    'Monday': 'Maandag', 'Tuesday': 'Dinsdag', 'Wednesday': 'Woensdag', 'Thursday': 'Donderdag',\n",
    "    'Friday': 'Vrijdag', 'Saturday': 'Zaterdag', 'Sunday': 'Zondag'\n",
    "}\n",
    "\n",
    "dim_date_df = pd.DataFrame({\n",
    "    'DateKey': date_list.strftime('%Y%m%d').astype(int),  # YYYYMMDD als key\n",
    "    'FullDate': date_list.date,  # Volledige datum\n",
    "    'MonthNameDutch': date_list.strftime('%B').map(months_translation),  # Maandnaam (kan vertaald worden)\n",
    "    'MonthNameEN': date_list.strftime('%B'),  # Maandnaam in Engels\n",
    "    'DayNameDutch': date_list.strftime('%A').map(days_translation),  # Dagnaam in Nederlands\n",
    "    'DayNameEN': date_list.strftime('%A'),  # Dagnaam in Engels\n",
    "    'QuarterName': 'Q' + date_list.quarter.astype(str),  # Kwartaal als 'Q1', 'Q2', ...\n",
    "    'QuarterNumber': date_list.quarter  # Kwartaalnummer (1-4)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Schrijf naar SQL Server\n",
    "dim_date_df.to_sql('DimDate', con=engine, if_exists='append', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_date_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: DimTime Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dim_time():\n",
    "    time_data = []\n",
    "\n",
    "    for hour in range(0, 24):\n",
    "        for minute in range(0, 60):\n",
    "            am_pm = 'AM' if hour < 12 else 'PM'\n",
    "            hour_12 = hour if 1 <= hour <= 12 else (12 if hour == 0 or hour == 24 else hour - 12)\n",
    "            time_key = f\"{hour:02}{minute:02}\"\n",
    "            full_time = f\"{hour:02}:{minute:02}:00\"\n",
    "            \n",
    "            time_data.append({\n",
    "                \"TimeKey\": time_key,\n",
    "                \"Hour\": hour_12,\n",
    "                \"Minutes\": minute,\n",
    "                \"FullTime\": full_time,\n",
    "                \"TimeAM_PM\": am_pm\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(time_data)\n",
    "\n",
    "# Data genereren\n",
    "dim_time_df = generate_dim_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data naar SQL Server schrijven\n",
    "dim_time_df.to_sql(\"DimTime\", con=engine, if_exists=\"append\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_time_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: DimWeatherStation Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lees de CSV voor weerstations\n",
    "weather_station_df = pd.read_csv('../data/input/aws_station.csv')\n",
    "\n",
    "# Verwerk de kolommen\n",
    "weather_station_df.rename(columns={\n",
    "    \"code\": \"WeatherStationID\",\n",
    "    \"name\": \"WeatherStationName\",\n",
    "    \"altitude\": \"Altitude\",\n",
    "    \"the_geom\": \"Coordinates\"\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Functie om Latitude en Longitude te extraheren uit 'the_geom' kolom\n",
    "def extract_lat_lon(geom):\n",
    "    match = re.search(r\"POINT \\(([\\d\\.-]+) ([\\d\\.-]+)\\)\", geom)\n",
    "    if match:\n",
    "        lon, lat = match.groups()\n",
    "        return float(lat), float(lon)\n",
    "    return None, None\n",
    "\n",
    "# Latitude en Longitude kolommen toevoegen\n",
    "weather_station_df[\"Latitude\"], weather_station_df[\"Longitude\"] = zip(*weather_station_df[\"Coordinates\"].apply(extract_lat_lon))\n",
    "\n",
    "# Onnodige kolom verwijderen\n",
    "weather_station_df.drop(columns=[\"Coordinates\"], inplace=True)\n",
    "weather_station_df = weather_station_df.drop(['FID', 'date_begin', 'date_end'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data naar SQL Server schrijven\n",
    "weather_station_df.to_sql(\"DimWeatherStation\", con=engine, if_exists=\"append\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Add a unique key column (starting from 1)\n",
    "weather_station_df.insert(0, 'WeatherStationKey', range(1, len(weather_station_df) + 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: FactWeather Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "file_path = \"../data/input/aws_1day.csv\"\n",
    "\n",
    "weather_data_df = pd.read_csv(file_path)\n",
    "\n",
    "weather_data_df.head(), weather_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_df = weather_data_df.drop(['FID', 'the_geom', 'qc_flags'], axis = 1)\n",
    "weather_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_df = weather_data_df.merge(weather_station_df, how='inner', left_on=\"code\", right_on='WeatherStationID')\n",
    "weather_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_df = weather_data_df.drop(['WeatherStationName', 'Latitude', 'Longitude', 'Altitude', 'WeatherStationID'], axis = 1)\n",
    "weather_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We converteren timestamp naar DateKey.\n",
    "weather_data_df['DateKey'] = weather_data_df['timestamp'].str[0:4] + weather_data_df['timestamp'].str[5:7] + weather_data_df['timestamp'].str[8:10]\n",
    "# DimTime toevoegen: eerst veld toevoegen, dan de waarden mergen.\n",
    "weather_data_df['Time'] = weather_data_df['timestamp'].str[-8:]\n",
    "weather_data_df = weather_data_df.merge(dim_time_df, how='inner', left_on=\"Time\", right_on='FullTime')\n",
    "\n",
    "# weather_data_df = weather_data_df.drop(['timestamp', 'Hour', 'Minutes', 'FullTime', 'TimeAM_PM'], axis = 1)\n",
    "\n",
    "weather_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we hernoemen de kolommen volgens de namen die in de DWH zitten. \n",
    "weather_data_df = weather_data_df.rename(columns={\"precip_quantity\": \"PrecipQuantity\",\"temp_avg\": \"TempAvg\",\"temp_max\": \"TempMax\",\"temp_min\": \"TempMin\",\n",
    "                                                  \"temp_grass_pt100_avg\": \"TempGrassPt100Avg\",\"temp_soil_avg\": \"TempSoilAvg\",\"temp_soil_avg_5cm\": \"TempSoilAvg5cm\",\n",
    "                                                  \"temp_soil_avg_10cm\": \"TempSoilAvg10cm\",\"temp_soil_avg_20cm\": \"TempSoilAvg20cm\",\n",
    "                                                  \"temp_soil_avg_50cm\": \"TempSoilAvg50cm\",\"wind_speed_10m\": \"WindSpeed10m\",\n",
    "                                                  \"wind_speed_avg_30m\": \"WindSpeedAvg30m\",\"wind_gusts_speed\": \"WindGustsSpeed\",\n",
    "                                                  \"humidity_rel_shelter_avg\": \"HumidityRelShelterAvg\",\"pressure\": \"Pressure\",\"sun_duration\": \"SunDuration\",\n",
    "                                                  \"short_wave_from_sky_avg\": \"ShortWaveFromSkyAvg\",\"sun_int_avg\": \"SunIntAvg\"})\n",
    "\n",
    "weather_data_df = weather_data_df.reindex(columns=[\"DateKey\", \"TimeKey\", \"WeatherStationKey\", \"PrecipQuantity\", \"TempAvg\", \"TempMax\", \"TempMin\",\n",
    "                                                    \"TempGrassPt100Avg\", \"TempSoilAvg\", \"TempSoilAvg5cm\", \"TempSoilAvg10cm\", \n",
    "                                                    \"TempSoilAvg20cm\", \"TempSoilAvg50cm\", \"WindSpeed10m\", \"WindSpeedAvg30m\", \n",
    "                                                    \"WindGustsSpeed\", \"HumidityRelShelterAvg\", \"Pressure\", \"SunDuration\", \"ShortWaveFromSkyAvg\", \n",
    "                                                    \"SunIntAvg\"])\n",
    "\n",
    "weather_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data naar SQL Server schrijven\n",
    "weather_data_df.to_sql(\"FactWeather\", con=engine, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5: FactBelpex Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Read the CSV with proper headers and separator\n",
    "belpex_df = pd.read_csv('../data/input/BelpexFilter.csv', encoding='Windows-1252', sep=';', names=['Date', 'BelpexPrice'], skiprows=1)\n",
    "\n",
    "# Clean and convert 'BelpexPrice'\n",
    "belpex_df['BelpexPrice'] = belpex_df['BelpexPrice'].str.replace('€', '', regex=False).str.replace(',', '.', regex=False).str.strip()\n",
    "belpex_df['BelpexPrice'] = pd.to_numeric(belpex_df['BelpexPrice'], errors='coerce')\n",
    "\n",
    "# Parse 'Date' into DateKey and FullTime using datetime\n",
    "belpex_df['Date'] = belpex_df['Date'].str.strip()  # Remove extra spaces\n",
    "belpex_df['DateKey'] = belpex_df['Date'].apply(lambda x: datetime.strptime(x.split()[0], '%d/%m/%Y').strftime('%Y%m%d'))\n",
    "belpex_df['FullTime'] = belpex_df['Date'].apply(lambda x: x.split()[1])  # Extract time part\n",
    "\n",
    "# Merge with DimTime to get TimeKey\n",
    "belpex_df = belpex_df.merge(dim_time_df[['FullTime', 'TimeKey']], how='inner', on='FullTime')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "belpex_df = belpex_df.drop(['FullTime', 'Date'], axis=1)\n",
    "\n",
    "# Write to SQL Server\n",
    "belpex_df.to_sql('FactBelpex', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6: FactNetworkCosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "csv_file_path = \"../data/input/Distributiekosten.csv\"\n",
    "df_distributiekosten = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Datum omzetten naar DateKey (YYYYMMDD)\n",
    "df_distributiekosten[\"DateKey\"] = df_distributiekosten[\"Van\"].apply(lambda x: datetime.strptime(x, \"%d/%m/%Y\").strftime(\"%Y%m%d\"))\n",
    "\n",
    "df_distributiekosten.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolommen hernoemen\n",
    "column_mapping = {\n",
    "    \"Capaciteitstarief_Digitale_meter\": \"CapacityTariff_DigitalMeter\",\n",
    "    \"Afnametarief_Digitale_meter_Normaal\": \"ConsumptionTariff_DigitalMeter_Normal\",\n",
    "    \"Afnametarief_Digitale_meter_Exclusief_nacht\": \"ConsumptionTariff_DigitalMeter_ExclusiveNight\",\n",
    "    \"Capaciteitstarief_Klassieke_meter\": \"CapacityTariff_ClassicMeter\",\n",
    "    \"Afnametarief_Klassieke_meter_Normaal\": \"ConsumptionTariff_ClassicMeter_Normal\",\n",
    "    \"Afnametarief_Klassieke_meter_Exclusief_nacht\": \"ConsumptionTariff_ClassicMeter_ExclusiveNight\",\n",
    "    \"Prosumententarief\": \"ProsumerTariff\",\n",
    "    \"Tarief_databeheer_Jaar_en_maandgelezen_meters\": \"DataManagementTariff_YearlyMonthlyReadMeters\",\n",
    "    \"Tarief_databeheer_Kwartiergelezen_meters\": \"DataManagementTariff_QuarterlyReadMeters\",\n",
    "    \"Intercommunale\": \"NetworkOperator\"\n",
    "}\n",
    "df_distributiekosten = df_distributiekosten.rename(columns=column_mapping)\n",
    "\n",
    "df_distributiekosten.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"DateKey\", \"NetworkOperator\", \"CapacityTariff_DigitalMeter\",\n",
    "    \"ConsumptionTariff_DigitalMeter_Normal\", \"ConsumptionTariff_DigitalMeter_ExclusiveNight\",\n",
    "    \"CapacityTariff_ClassicMeter\", \"ConsumptionTariff_ClassicMeter_Normal\",\n",
    "    \"ConsumptionTariff_ClassicMeter_ExclusiveNight\", \"ProsumerTariff\",\n",
    "    \"DataManagementTariff_YearlyMonthlyReadMeters\", \"DataManagementTariff_QuarterlyReadMeters\"\n",
    "]\n",
    "df_distributiekosten = df_distributiekosten[columns]\n",
    "\n",
    "df_distributiekosten.to_sql('FactNetworkCosts', con=engine, if_exists='append', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6: Verbruikersdata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dowmload de dataset: https://opendata.fluvius.be/explore/dataset/1_50-verbruiksprofielen-dm-elek-kwartierwaarden-voor-een-volledig-jaar/information/\n",
    "# Lees de CSV met alle verbruikersdata in een pandas dataframe.\n",
    "csv_file_path = \"../data/input/P6269_1_50_DMK_Sample_Elek.csv\"\n",
    "df_verbruikersdata = pd.read_csv(csv_file_path, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hernoem kolom om te matchen met tabel in sql server\n",
    "df_verbruikersdata.rename(columns={\n",
    "    'PV-Installatie_Indicator': 'PV_Installatie_Indicator',\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_verbruikersdata.to_sql('DimUser_Staging', con=engine, if_exists='append', index=False, chunksize=5000)\n",
    "\n",
    "# Laatste stap, voer sql script uit: ../DWH/Staging_To_DimUser.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7: Overige Tabellen\n",
    "\n",
    "- Voor de overige tabellen, volg dezelfde logica:\n",
    "    - Lees de CSV’s.\n",
    "    - Voeg de benodigde foreign keys toe.\n",
    "    - Schrijf de data weg naar de juiste tabellen via bulk-insert of andere batch methoden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algemeen:\n",
    "Voor alle bulk-insert taken moet je zorgen voor een efficiënte schrijfmethode naar SQL Server, bijvoorbeeld:\n",
    "\n",
    "- to_sql() in combinatie met een SQLAlchemy engine.\n",
    "- Bulk-insert via pyodbc of tools zoals bcp.\n",
    "- Gebruik maken van BULK INSERT in SQL Server voor het snel inladen van grote datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Voeg contract types toe:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a list of energy contracts with their types\n",
    "energy_contracts = [\n",
    "    # BOLT\n",
    "    {'Provider': 'BOLT', 'ContractName': 'ELEKTRICITEIT', 'ContractType': 'Variabel'},\n",
    "    {'Provider': 'BOLT', 'ContractName': 'VAST', 'ContractType': 'Vast'},\n",
    "    \n",
    "    # DATS24\n",
    "    {'Provider': 'DATS24', 'ContractName': 'ELEKTRICITEIT', 'ContractType': 'Variabel'},\n",
    "    \n",
    "    # ENECO\n",
    "    {'Provider': 'ENECO', 'ContractName': 'ZON WIND FLEX', 'ContractType': 'Variabel'},\n",
    "    \n",
    "    # ENERGIE-BE\n",
    "    {'Provider': 'ENERGIE-BE', 'ContractName': 'VARIABEL', 'ContractType': 'Variabel'},\n",
    "    {'Provider': 'ENERGIE-BE', 'ContractName': 'VAST', 'ContractType': 'Vast'},\n",
    "    \n",
    "    # LUMINUS\n",
    "    {'Provider': 'LUMINUS', 'ContractName': 'DYNAMIC', 'ContractType': 'Dynamisch'},\n",
    "    \n",
    "    # OCTA+\n",
    "    {'Provider': 'OCTA+', 'ContractName': 'DYNAMIC', 'ContractType': 'Dynamisch'},\n",
    "    {'Provider': 'OCTA+', 'ContractName': 'ECO CLEAR', 'ContractType': 'Variabel'},\n",
    "    {'Provider': 'OCTA+', 'ContractName': 'FIXED', 'ContractType': 'Vast'},\n",
    "    {'Provider': 'OCTA+', 'ContractName': 'SMART VARIABEL', 'ContractType': 'Variabel'},\n",
    "    \n",
    "    # TOTALENERGIES\n",
    "    {'Provider': 'TOTALENERGIES', 'ContractName': 'PIXEL', 'ContractType': 'Variabel'},\n",
    "    {'Provider': 'TOTALENERGIES', 'ContractName': 'PIXEL NEXT VAST', 'ContractType': 'Vast'},\n",
    "    {'Provider': 'TOTALENERGIES', 'ContractName': 'PIXEL EDRIVE', 'ContractType': 'Variabel'},\n",
    "    {'Provider': 'TOTALENERGIES', 'ContractName': 'PIXIE', 'ContractType': 'Variabel'}\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "dim_energy_contract_df = pd.DataFrame(energy_contracts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display the DataFrame for verification\n",
    "print(dim_energy_contract_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Write to SQL Server\n",
    "dim_energy_contract_df.to_sql('DimEnergyContract', con=engine, if_exists='append', index=False)\n",
    "\n",
    "print(f\"Successfully inserted {len(dim_energy_contract_df)} energy contracts into DimEnergyContract table.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Voeg tariefkaarten toe:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the CSV file with semicolon delimiter\n",
    "csv_file_path = '../data/input/energy_costs.csv'\n",
    "df = pd.read_csv(csv_file_path, sep=';')\n",
    "\n",
    "# Get provider and contract name from EnergyCostKey\n",
    "df['Provider'] = df['EnergyCostKey'].apply(lambda x: x.split('_')[1])\n",
    "df['ContractName'] = df['EnergyCostKey'].apply(lambda x: x.split('_')[2])\n",
    "\n",
    "# Convert DateKey from 'YYYY-MM-DD' to integer format 'YYYYMMDD'\n",
    "df['DateKey'] = df['DateKey'].apply(lambda x: int(x.replace('-', '')))\n",
    "\n",
    "# Replace empty strings with None for proper SQL NULL values\n",
    "df = df.replace('', None)\n",
    "\n",
    "# Look up ContractKey from DimEnergyContract table\n",
    "# First, get the existing contracts from the database\n",
    "contracts_query = \"SELECT ContractKey, Provider, ContractName FROM DimEnergyContract\"\n",
    "contracts_df = pd.read_sql(contracts_query, engine)\n",
    "\n",
    "# Create a dictionary mapping (Provider, ContractName) to ContractKey\n",
    "contract_keys = {}\n",
    "for _, row in contracts_df.iterrows():\n",
    "    contract_keys[(row['Provider'], row['ContractName'])] = row['ContractKey']\n",
    "\n",
    "# Map the ContractKey to each row\n",
    "df['ContractKey'] = df.apply(lambda row: contract_keys.get((row['Provider'], row['ContractName'])), axis=1)\n",
    "\n",
    "# Drop the original EnergyCostKey and the temporary Provider and ContractName columns\n",
    "df = df.drop(['EnergyCostKey', 'Provider', 'ContractName'], axis=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display the first few rows to verify the transformation\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " # Insert the data into the SQL Server database\n",
    "df.to_sql('FactEnergyCost', con=engine, if_exists='append', index=False)\n",
    "\n",
    "print(f\"Successfully inserted {len(df)} rows into FactEnergyCost table.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
