{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: DimDate Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade sqlalchemy\n",
    "%pip install --upgrade pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "print(pyodbc.drivers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gegevens voor de verbinding\n",
    "server = \"ARES\"  # Servernaam of IP-adres van je SQL Server\n",
    "database = \"DEP1_DWH\"  # Naam van je database\n",
    "\n",
    "# Maak de verbindingsstring met Windows Authenticatie (Integrated Security)\n",
    "engine = create_engine(\"mssql+pyodbc://@{}/{}?driver=ODBC+Driver+17+for+SQL+Server\".format(server, database))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maak een range van datums\n",
    "date_list = pd.date_range(start=\"01-01-2010\", end=\"31-12-2025\", freq='D')\n",
    "\n",
    "dim_date_df = pd.DataFrame({\n",
    "    'DateKey': date_list.strftime('%Y%m%d').astype(int),  # YYYYMMDD als key\n",
    "    'FullDate': date_list.date,  # Volledige datum\n",
    "    'DayName': date_list.strftime('%A'),  # Dagnaam in Engels\n",
    "    'MonthNameDutch': date_list.strftime('%B'),  # Maandnaam (kan vertaald worden)\n",
    "    'MonthNameEN': date_list.strftime('%B'),  # Maandnaam in Engels\n",
    "    'DayNameDutch': date_list.strftime('%A'),  # Dagnaam in Nederlands\n",
    "    'DayNameEN': date_list.strftime('%A'),  # Dagnaam in Engels\n",
    "    'QuarterName': 'Q' + date_list.quarter.astype(str),  # Kwartaal als 'Q1', 'Q2', ...\n",
    "    'QuarterNumber': date_list.quarter  # Kwartaalnummer (1-4)\n",
    "})\n",
    "\n",
    "# Schrijf naar SQL Server\n",
    "dim_date_df.to_sql('DimDate', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_date_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: DimTime Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dim_time():\n",
    "    time_data = []\n",
    "    s=0\n",
    "\n",
    "    for hour in range(0, 24):\n",
    "        for minute in range(0, 60):\n",
    "            am_pm = 'AM' if hour < 12 else 'PM'\n",
    "            hour_12 = hour if 1 <= hour <= 12 else (12 if hour == 0 or hour == 24 else hour - 12)\n",
    "            time_key = f\"{hour:02}{minute:02}\"\n",
    "            full_time = f\"{hour:02}:{minute:02}:{s:02}\"\n",
    "            \n",
    "            time_data.append({\n",
    "                \"TimeKey\": time_key,\n",
    "                \"Hour\": hour_12,\n",
    "                \"Minutes\": minute,\n",
    "                \"FullTime\": full_time,\n",
    "                \"TimeAM_PM\": am_pm\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(time_data)\n",
    "\n",
    "# Data genereren\n",
    "dim_time_df = generate_dim_time()\n",
    "\n",
    "# Data naar SQL Server schrijven\n",
    "dim_time_df.to_sql(\"DimTime\", con=engine, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: DimWeatherStation Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lees de CSV voor weerstations\n",
    "weather_station_df = pd.read_csv('/data/input/aws_station.csv')\n",
    "\n",
    "# Verwerk de kolommen\n",
    "weather_station_df.rename(columns={\n",
    "    \"code\": \"WeatherStationID\",\n",
    "    \"name\": \"WeatherStationName\",\n",
    "    \"altitude\": \"Altitude\",\n",
    "    \"the_geom\": \"Coordinates\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Functie om Latitude en Longitude te extraheren uit 'the_geom' kolom\n",
    "def extract_lat_lon(geom):\n",
    "    match = re.search(r\"POINT \\(([\\d\\.-]+) ([\\d\\.-]+)\\)\", geom)\n",
    "    if match:\n",
    "        lon, lat = match.groups()\n",
    "        return float(lat), float(lon)\n",
    "    return None, None\n",
    "\n",
    "# Latitude en Longitude kolommen toevoegen\n",
    "weather_station_df[\"Latitude\"], weather_station_df[\"Longitude\"] = zip(*weather_station_df[\"Coordinates\"].apply(extract_lat_lon))\n",
    "\n",
    "# Onnodige kolom verwijderen\n",
    "weather_station_df.drop(columns=[\"Coordinates\"], inplace=True)\n",
    "\n",
    "# Data naar SQL Server schrijven\n",
    "weather_station_df.to_sql(\"DimWeatherStation\", con=engine, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: FactWeather Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lees de CSV voor weerdata\n",
    "weather_data_df = pd.read_csv('/data/input/aws_1day.csv')\n",
    "\n",
    "weather_data_df = weather_data_df.drop(['FID', 'the_geom', 'qc_flags'], axis = 1)\n",
    "weather_data_df = weather_data_df.merge(weather_station_df, how='inner', left_on=\"code\", right_on='WeatherStationID')\n",
    "weather_data_df = weather_data_df.drop(['Name', 'Point', 'Latitude', 'Longitude', 'Altitude', 'WeatherStationID'], axis = 1)\n",
    "weather_data_df['DateKey'] = weather_data_df['timestamp'].str[0:4] + weather_data_df['timestamp'].str[5:7] + weather_data_df['timestamp'].str[8:10]\n",
    "weather_data_df['Time'] = weather_data_df['timestamp'].str[-8:]\n",
    "weather_data_df = weather_data_df.merge(dim_time_df, how='inner', left_on=\"Time\", right_on='FullTime')\n",
    "weather_data_df = weather_data_df.drop(['timestamp', 'Hour', 'Minutes', 'FullTime', 'TimeAM_PM'], axis = 1)\n",
    "\n",
    "weather_data_df = weather_data_df.rename(columns={\"precip\": \"PrecipQuantity\",\"temp_avg\": \"TempAvg\",\"temp_max\": \"TempMax\",\"temp_min\": \"TempMin\",\n",
    "                                                  \"temp_grass\": \"TempGrassPt100Avg\",\"temp_soil_avg\": \"TempSoilAvg\",\"temp_soil_5cm\": \"TempSoilAvg5cm\",\n",
    "                                                  \"temp_soil_10cm\": \"TempSoilTempSoilAvg10cm\",\"temp_soil_20cm\": \"TempSoilTempSoilAvg20cm\",\n",
    "                                                  \"temp_soil_50cm\": \"TempSoilTempSoilAvg50cm\",\"wind_speed_10m\": \"WindSpeed10m\",\n",
    "                                                  \"wind_speed_avg_30m\": \"WindSpeedAvg30m\",\"wind_gust_speed\": \"WindGustsSpeed\",\n",
    "                                                  \"humidity_avg\": \"HumidityRelShelterAvg\",\"pressure\": \"Pressure\",\"sun_duration\": \"SunDuration\",\n",
    "                                                  \"short_wave\": \"ShortWaveFromSkyAvg\",\"sun_intensity\": \"SunIntAvg\"})\n",
    "\n",
    "weather_data_df = weather_data_df.reindex(columns=[\"DateKey\", \"TimeKey\", \"WeatherStationKey\", \"PrecipQuantity\", \"TempAvg\", \"TempMax\", \"TempMin\",\n",
    "                                                    \"TempGrassPt100Avg\", \"TempSoilAvg\", \"TempSoilAvg5cm\", \"TempSoilTempSoilAvg10cm\", \n",
    "                                                    \"TempSoilTempSoilAvg20cm\", \"TempSoilTempSoilAvg50cm\", \"WindSpeed10m\", \"WindSpeedAvg30m\", \n",
    "                                                    \"WindGustsSpeed\", \"HumidityRelShelterAvg\", \"Pressure\", \"SunDuration\", \"ShortWaveFromSkyAvg\", \n",
    "                                                    \"SunIntAvg\"])\n",
    "\n",
    "# Data naar SQL Server schrijven\n",
    "weather_data_df.to_sql(\"FactWeather\", con=engine, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5: FactBelpex Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lees de CSV voor BELPEX\n",
    "belpex_df = pd.read_csv('/data/input/BelpexFilter.csv')\n",
    "\n",
    "#TODO\n",
    "belpex_df['DateKey'] = belpex_df['Date'].str[5:9] + belpex_df['Date'].str[5:7] + belpex_df['Date'].str[8:10]\n",
    "belpex_df['Time'] = belpex_df['Date'].str[-8:]\n",
    "belpex_df = belpex_df.merge(dim_time_df, how='inner', left_on=\"Time\", right_on='FullTime')\n",
    "belpex_df = belpex_df.drop(['Hour', 'Minutes', 'FullTime', 'TimeAM_PM', 'Date'], axis = 1)\n",
    "\n",
    "# Schrijf naar SQL Server\n",
    "belpex_df.to_sql('FactBelpex', con=engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6: Verbruikersdata\n",
    "\n",
    "- Verwerk de verbruikersdata naar het juiste formaat.\n",
    "- Gebruik SQL Server's bulk-insert om grote hoeveelheden data efficiënt te verwerken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7: Overige Tabellen\n",
    "\n",
    "- Voor de overige tabellen, volg dezelfde logica:\n",
    "    - Lees de CSV’s.\n",
    "    - Voeg de benodigde foreign keys toe.\n",
    "    - Schrijf de data weg naar de juiste tabellen via bulk-insert of andere batch methoden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algemeen:\n",
    "Voor alle bulk-insert taken moet je zorgen voor een efficiënte schrijfmethode naar SQL Server, bijvoorbeeld:\n",
    "\n",
    "- to_sql() in combinatie met een SQLAlchemy engine.\n",
    "- Bulk-insert via pyodbc of tools zoals bcp.\n",
    "- Gebruik maken van BULK INSERT in SQL Server voor het snel inladen van grote datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
